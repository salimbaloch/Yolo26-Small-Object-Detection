{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1207642,"sourceType":"datasetVersion","datasetId":688943}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/salimbaloch/yolo26-small-object-detection?scriptVersionId=294986747\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 1: Install Dependencies and Setup\n# ============================================================================\n!pip install ultralytics huggingface_hub -q\n!pip install xmltodict -q\n\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport xmltodict\nimport shutil\nfrom tqdm import tqdm\nfrom huggingface_hub import hf_hub_download\nfrom ultralytics import YOLO\n\nprint(\"âœ“ Libraries imported successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:20:18.828841Z","iopub.execute_input":"2026-01-28T15:20:18.829508Z","iopub.status.idle":"2026-01-28T15:20:35.905862Z","shell.execute_reply.started":"2026-01-28T15:20:18.829471Z","shell.execute_reply":"2026-01-28T15:20:35.905145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: Download YOLOv26-OBB Model\n# ============================================================================\nprint(\"Downloading YOLOv26-OBB model...\")\nmodel_path = hf_hub_download(\n    repo_id=\"openvision/yolo26-n-obb\", \n    filename=\"model.pt\",\n    local_dir=\"./models\"\n)\nprint(f\"âœ“ Model downloaded to: {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:20:39.610321Z","iopub.execute_input":"2026-01-28T15:20:39.610813Z","iopub.status.idle":"2026-01-28T15:20:41.983638Z","shell.execute_reply.started":"2026-01-28T15:20:39.610774Z","shell.execute_reply":"2026-01-28T15:20:41.982929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 3: Dataset Structure Analysis\n# ============================================================================\n# Kaggle dataset path\ndataset_path = \"/kaggle/input/ship-detection\"\n\n# Explore structure\nprint(\"\\n=== Dataset Structure ===\")\nfor root, dirs, files in os.walk(dataset_path):\n    level = root.replace(dataset_path, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f'{indent}{os.path.basename(root)}/')\n    subindent = ' ' * 2 * (level + 1)\n    for file in files[:3]:  # Show first 3 files\n        print(f'{subindent}{file}')\n    if len(files) > 3:\n        print(f'{subindent}... and {len(files)-3} more files')\n\n# Count files\nimages_path = f\"{dataset_path}/images\"\nannotations_path = f\"{dataset_path}/annotations\"\n\nnum_images = len([f for f in os.listdir(images_path) if f.endswith(('.jpg', '.png'))])\nnum_annotations = len([f for f in os.listdir(annotations_path) if f.endswith('.xml')])\n\nprint(f\"\\nâœ“ Total images: {num_images}\")\nprint(f\"âœ“ Total annotations: {num_annotations}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:20:42.999193Z","iopub.execute_input":"2026-01-28T15:20:42.999539Z","iopub.status.idle":"2026-01-28T15:20:45.354329Z","shell.execute_reply.started":"2026-01-28T15:20:42.999509Z","shell.execute_reply":"2026-01-28T15:20:45.353456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 4: Parse Pascal VOC Annotations\n# ============================================================================\ndef parse_voc_annotation(xml_file):\n    \"\"\"\n    Parse Pascal VOC XML annotation and extract bounding boxes\n    Returns: list of dicts with bbox coordinates and class\n    \"\"\"\n    with open(xml_file, 'r') as f:\n        xml_dict = xmltodict.parse(f.read())\n    \n    objects = []\n    annotation = xml_dict['annotation']\n    \n    # Get image dimensions\n    size = annotation['size']\n    img_width = int(size['width'])\n    img_height = int(size['height'])\n    \n    # Handle single object vs multiple objects\n    obj_list = annotation.get('object', [])\n    if not isinstance(obj_list, list):\n        obj_list = [obj_list]\n    \n    for obj in obj_list:\n        bbox = obj['bndbox']\n        objects.append({\n            'class': obj['name'],\n            'xmin': int(bbox['xmin']),\n            'ymin': int(bbox['ymin']),\n            'xmax': int(bbox['xmax']),\n            'ymax': int(bbox['ymax']),\n            'img_width': img_width,\n            'img_height': img_height\n        })\n    \n    return objects\n\n# Test parsing\nsample_xml = f\"{annotations_path}/{os.listdir(annotations_path)[0]}\"\nsample_objs = parse_voc_annotation(sample_xml)\nprint(\"\\n=== Sample Annotation ===\")\nprint(f\"File: {os.path.basename(sample_xml)}\")\nprint(f\"Objects detected: {len(sample_objs)}\")\nprint(f\"First object: {sample_objs[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:20:46.839079Z","iopub.execute_input":"2026-01-28T15:20:46.839382Z","iopub.status.idle":"2026-01-28T15:20:46.849645Z","shell.execute_reply.started":"2026-01-28T15:20:46.839353Z","shell.execute_reply":"2026-01-28T15:20:46.849022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: Convert VOC to YOLO-OBB Format\n# ============================================================================\ndef voc_to_yolo_obb(objects, img_width, img_height):\n    \"\"\"\n    Convert Pascal VOC bbox to YOLO-OBB format\n    YOLO-OBB format: class_id x1 y1 x2 y2 x3 y3 x4 y4\n    All coordinates normalized [0-1]\n    \n    For horizontal bboxes, we create a rotated bbox with 0Â° rotation\n    \"\"\"\n    yolo_lines = []\n    \n    for obj in objects:\n        # Class ID (0 for ship - single class)\n        class_id = 0\n        \n        # Normalize coordinates\n        xmin = obj['xmin'] / img_width\n        ymin = obj['ymin'] / img_height\n        xmax = obj['xmax'] / img_width\n        ymax = obj['ymax'] / img_height\n        \n        # Create 4 corner points for OBB (clockwise from top-left)\n        # For horizontal bbox: TL, TR, BR, BL\n        x1, y1 = xmin, ymin  # Top-left\n        x2, y2 = xmax, ymin  # Top-right\n        x3, y3 = xmax, ymax  # Bottom-right\n        x4, y4 = xmin, ymax  # Bottom-left\n        \n        # YOLO-OBB format: class x1 y1 x2 y2 x3 y3 x4 y4\n        yolo_line = f\"{class_id} {x1:.6f} {y1:.6f} {x2:.6f} {y2:.6f} {x3:.6f} {y3:.6f} {x4:.6f} {y4:.6f}\"\n        yolo_lines.append(yolo_line)\n    \n    return yolo_lines\n\n# Test conversion\nsample_yolo = voc_to_yolo_obb(sample_objs, sample_objs[0]['img_width'], sample_objs[0]['img_height'])\nprint(\"\\n=== YOLO-OBB Format (sample) ===\")\nprint(sample_yolo[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:20:49.326892Z","iopub.execute_input":"2026-01-28T15:20:49.32767Z","iopub.status.idle":"2026-01-28T15:20:49.334267Z","shell.execute_reply.started":"2026-01-28T15:20:49.327635Z","shell.execute_reply":"2026-01-28T15:20:49.333388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 6: Create YOLO Dataset Structure\n# ============================================================================\n# Create directories\noutput_dir = \"/kaggle/working/ship_dataset\"\nfor split in ['train', 'val', 'test']:\n    os.makedirs(f\"{output_dir}/images/{split}\", exist_ok=True)\n    os.makedirs(f\"{output_dir}/labels/{split}\", exist_ok=True)\n\nprint(\"âœ“ Dataset directories created\")\n\n# Get all image files\nall_images = sorted([f for f in os.listdir(images_path) if f.endswith(('.jpg', '.png'))])\ntotal_images = len(all_images)\n\n# Split: 70% train, 15% val, 15% test\ntrain_split = int(0.7 * total_images)\nval_split = int(0.85 * total_images)\n\ntrain_images = all_images[:train_split]\nval_images = all_images[train_split:val_split]\ntest_images = all_images[val_split:]\n\nprint(f\"\\n=== Dataset Split ===\")\nprint(f\"Train: {len(train_images)} images\")\nprint(f\"Val: {len(val_images)} images\")\nprint(f\"Test: {len(test_images)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:20:50.714558Z","iopub.execute_input":"2026-01-28T15:20:50.715299Z","iopub.status.idle":"2026-01-28T15:20:50.723117Z","shell.execute_reply.started":"2026-01-28T15:20:50.715265Z","shell.execute_reply":"2026-01-28T15:20:50.722456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 7: Process and Convert Dataset\n# ============================================================================\ndef process_split(image_list, split_name):\n    \"\"\"Process images and annotations for a given split\"\"\"\n    print(f\"\\nProcessing {split_name} split...\")\n    \n    successful = 0\n    failed = 0\n    \n    for img_name in tqdm(image_list):\n        try:\n            # Get corresponding XML file\n            base_name = os.path.splitext(img_name)[0]\n            xml_file = f\"{annotations_path}/{base_name}.xml\"\n            \n            if not os.path.exists(xml_file):\n                failed += 1\n                continue\n            \n            # Parse annotation\n            objects = parse_voc_annotation(xml_file)\n            \n            if len(objects) == 0:\n                failed += 1\n                continue\n            \n            # Convert to YOLO-OBB format\n            img_width = objects[0]['img_width']\n            img_height = objects[0]['img_height']\n            yolo_lines = voc_to_yolo_obb(objects, img_width, img_height)\n            \n            # Copy image\n            src_img = f\"{images_path}/{img_name}\"\n            dst_img = f\"{output_dir}/images/{split_name}/{img_name}\"\n            shutil.copy(src_img, dst_img)\n            \n            # Save label\n            label_file = f\"{output_dir}/labels/{split_name}/{base_name}.txt\"\n            with open(label_file, 'w') as f:\n                f.write('\\n'.join(yolo_lines))\n            \n            successful += 1\n            \n        except Exception as e:\n            print(f\"Error processing {img_name}: {e}\")\n            failed += 1\n    \n    print(f\"âœ“ {split_name}: {successful} successful, {failed} failed\")\n    return successful, failed\n\n# Process all splits\ntrain_success, train_fail = process_split(train_images, 'train')\nval_success, val_fail = process_split(val_images, 'val')\ntest_success, test_fail = process_split(test_images, 'test')\n\nprint(\"\\n=== Conversion Complete ===\")\nprint(f\"Total successful: {train_success + val_success + test_success}\")\nprint(f\"Total failed: {train_fail + val_fail + test_fail}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:20:52.108087Z","iopub.execute_input":"2026-01-28T15:20:52.10871Z","iopub.status.idle":"2026-01-28T15:20:57.080577Z","shell.execute_reply.started":"2026-01-28T15:20:52.10868Z","shell.execute_reply":"2026-01-28T15:20:57.079898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 8: Create Dataset YAML Configuration\n# ============================================================================\nyaml_content = f\"\"\"# Ship Detection Dataset - YOLOv26-OBB\npath: {output_dir}\ntrain: images/train\nval: images/val\ntest: images/test\n\n# Classes\nnames:\n  0: ship\n\n# Number of classes\nnc: 1\n\"\"\"\n\nyaml_path = f\"{output_dir}/data.yaml\"\nwith open(yaml_path, 'w') as f:\n    f.write(yaml_content)\n\nprint(f\"âœ“ Dataset YAML created at: {yaml_path}\")\nprint(\"\\n\" + yaml_content)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:21:00.211005Z","iopub.execute_input":"2026-01-28T15:21:00.211315Z","iopub.status.idle":"2026-01-28T15:21:00.21699Z","shell.execute_reply.started":"2026-01-28T15:21:00.211288Z","shell.execute_reply":"2026-01-28T15:21:00.216245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 9: Visualize Sample Annotations\n# ============================================================================\ndef visualize_obb_annotation(img_path, label_path):\n    \"\"\"Visualize image with OBB annotations\"\"\"\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    h, w = img.shape[:2]\n    \n    # Read labels\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    \n    for line in lines:\n        parts = line.strip().split()\n        class_id = int(parts[0])\n        \n        # Get 4 corner points (denormalize)\n        points = []\n        for i in range(1, 9, 2):\n            x = float(parts[i]) * w\n            y = float(parts[i+1]) * h\n            points.append([int(x), int(y)])\n        \n        points = np.array(points, dtype=np.int32)\n        \n        # Draw polygon\n        cv2.polylines(img, [points], True, (0, 255, 0), 2)\n        \n        # Draw corner points\n        for point in points:\n            cv2.circle(img, tuple(point), 3, (255, 0, 0), -1)\n        \n        # Add label\n        cv2.putText(img, 'ship', tuple(points[0]), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n    \n    return img\n\n# Visualize samples from each split\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Sample Annotations - YOLOv26-OBB Format', fontsize=16)\n\nfor idx, split in enumerate(['train', 'val', 'test']):\n    split_imgs = os.listdir(f\"{output_dir}/images/{split}\")[:2]\n    \n    for row, img_name in enumerate(split_imgs):\n        img_path = f\"{output_dir}/images/{split}/{img_name}\"\n        label_path = f\"{output_dir}/labels/{split}/{os.path.splitext(img_name)[0]}.txt\"\n        \n        if os.path.exists(img_path) and os.path.exists(label_path):\n            img_viz = visualize_obb_annotation(img_path, label_path)\n            axes[row, idx].imshow(img_viz)\n            axes[row, idx].set_title(f'{split.upper()} - {img_name}')\n            axes[row, idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"âœ“ Visualization complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:21:02.305929Z","iopub.execute_input":"2026-01-28T15:21:02.306508Z","iopub.status.idle":"2026-01-28T15:21:03.599003Z","shell.execute_reply.started":"2026-01-28T15:21:02.306468Z","shell.execute_reply":"2026-01-28T15:21:03.597844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 10: Initialize and Train YOLOv26-OBB\n# ============================================================================\nprint(\"\\n=== Training YOLOv26-OBB ===\")\n\n# Load model\nmodel = YOLO(model_path)\n\n# Training configuration\ntrain_args = {\n    'data': yaml_path,\n    'epochs': 50,           # Increase for better results\n    'imgsz': 640,\n    'batch': 16,            # Adjust based on GPU memory\n    'device': 0,            # Use GPU\n    'project': '/kaggle/working/runs',\n    'name': 'ship_yolov26_obb',\n    'patience': 10,\n    'save': True,\n    'plots': True,\n    'val': True,\n    'verbose': True,\n    \n    # Augmentation\n    'hsv_h': 0.015,\n    'hsv_s': 0.7,\n    'hsv_v': 0.4,\n    'degrees': 10.0,        # Rotation augmentation\n    'translate': 0.1,\n    'scale': 0.5,\n    'flipud': 0.5,          # Vertical flip (ships can appear upside down in aerial)\n    'fliplr': 0.5,          # Horizontal flip\n    'mosaic': 1.0,\n    \n    # Optimization\n    'optimizer': 'AdamW',\n    'lr0': 0.001,\n    'lrf': 0.01,\n    'momentum': 0.937,\n    'weight_decay': 0.0005,\n    'warmup_epochs': 3.0,\n    'warmup_momentum': 0.8,\n    'warmup_bias_lr': 0.1,\n}\n\n# Start training\nprint(\"\\nStarting training...\")\nresults = model.train(**train_args)\n\nprint(\"\\nâœ“ Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:30:26.070972Z","iopub.execute_input":"2026-01-28T15:30:26.071324Z","iopub.status.idle":"2026-01-28T15:38:56.576203Z","shell.execute_reply.started":"2026-01-28T15:30:26.071292Z","shell.execute_reply":"2026-01-28T15:38:56.575507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 11: Evaluate on Validation Set (FIXED VERSION)\n# ============================================================================\nprint(\"\\n=== Validation Metrics ===\")\n\n# Find the actual model from training results\nimport glob\nmodel_dirs = glob.glob('/kaggle/working/runs/ship_yolov26_obb*')\nif model_dirs:\n    # Get the most recent run (highest number)\n    latest_run = sorted(model_dirs, key=lambda x: int(x.split('ship_yolov26_obb')[-1]) if x.split('ship_yolov26_obb')[-1].isdigit() else 0)[-1]\n    best_model_path = f\"{latest_run}/weights/best.pt\"\n    print(f\"Found model at: {best_model_path}\")\nelse:\n    # Fallback to what you expected\n    best_model_path = '/kaggle/working/runs/ship_yolov26_obb/weights/best.pt'\n\n# Load best model\nprint(f\"Loading model: {best_model_path}\")\nmodel_best = YOLO(best_model_path)\n\n# Validate\nval_results = model_best.val(\n    data=yaml_path,\n    split='val',\n    imgsz=640,\n    batch=16,  # Add this to match training\n    plots=True,\n    save_json=True\n)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"VALIDATION RESULTS\")\nprint(\"=\"*50)\n\n# Extract and print metrics\nif hasattr(val_results, 'box'):\n    map50 = val_results.box.map50\n    map50_95 = val_results.box.map\n    \n    # Handle precision/recall which might be arrays\n    if isinstance(val_results.box.p, np.ndarray):\n        precision = val_results.box.p.mean()\n    else:\n        precision = val_results.box.p\n    \n    if isinstance(val_results.box.r, np.ndarray):\n        recall = val_results.box.r.mean()\n    else:\n        recall = val_results.box.r\n    \n    print(f\"mAP50: {map50:.4f}\")\n    print(f\"mAP50-95: {map50_95:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\nelse:\n    print(\"No validation results found. Check your validation parameters.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:01:29.59659Z","iopub.execute_input":"2026-01-28T16:01:29.5972Z","iopub.status.idle":"2026-01-28T16:01:34.879101Z","shell.execute_reply.started":"2026-01-28T16:01:29.597162Z","shell.execute_reply":"2026-01-28T16:01:34.878374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.path.exists('/kaggle/working/runs/ship_yolov26_obb6/weights/best.pt'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n Running Predictions on Test Set\")\n\ntest_imgs = f\"{output_dir}/images/test\"\ntest_results = model_best.predict(\n    source=test_imgs,\n    save=True,\n    conf=0.25,\n    iou=0.45,\n    imgsz=640,\n    project='/kaggle/working/runs',\n    name='test_predictions',\n    save_txt=True,\n    save_conf=True\n)\n\nprint(f\" Predictions saved to: /kaggle/working/runs/test_predictions\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:04:54.864704Z","iopub.execute_input":"2026-01-28T16:04:54.865553Z","iopub.status.idle":"2026-01-28T16:04:57.465459Z","shell.execute_reply.started":"2026-01-28T16:04:54.865517Z","shell.execute_reply":"2026-01-28T16:04:57.46487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_predictions(num_samples=6):\n    \n    pred_dir = '/kaggle/working/runs/test_predictions'\n    pred_images = [f for f in os.listdir(pred_dir) if f.endswith(('.jpg', '.png'))][:num_samples]\n    \n    rows = (num_samples + 2) // 3\n    fig, axes = plt.subplots(rows, 3, figsize=(18, 6*rows))\n    axes = axes.flatten() if num_samples > 1 else [axes]\n    \n    for idx, img_name in enumerate(pred_images):\n        img_path = f\"{pred_dir}/{img_name}\"\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        axes[idx].imshow(img)\n        axes[idx].set_title(f'Prediction: {img_name}')\n        axes[idx].axis('off')\n    \n    # Hide empty subplots\n    for idx in range(len(pred_images), len(axes)):\n        axes[idx].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_predictions(num_samples=9)\nprint(\"Predictions visualized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:05:01.766065Z","iopub.execute_input":"2026-01-28T16:05:01.766388Z","iopub.status.idle":"2026-01-28T16:05:03.539437Z","shell.execute_reply.started":"2026-01-28T16:05:01.76636Z","shell.execute_reply":"2026-01-28T16:05:03.538592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_obb_info(results):\n   \n    detections = []\n    \n    for result in results:\n        img_name = Path(result.path).name\n        \n        if result.obb is not None and len(result.obb) > 0:\n            # Get OBB data\n            boxes = result.obb.xyxyxyxy.cpu().numpy()  # 4 corner points\n            confs = result.obb.conf.cpu().numpy()\n            classes = result.obb.cls.cpu().numpy()\n            \n            # Extract rotation angles if available\n            if hasattr(result.obb, 'data'):\n                angles = result.obb.data[:, -1].cpu().numpy() if result.obb.data.shape[1] > 5 else None\n            else:\n                angles = None\n            \n            for i in range(len(boxes)):\n                detection = {\n                    'image': img_name,\n                    'class': 'ship',\n                    'confidence': float(confs[i]),\n                    'x1': float(boxes[i][0][0]),\n                    'y1': float(boxes[i][0][1]),\n                    'x2': float(boxes[i][1][0]),\n                    'y2': float(boxes[i][1][1]),\n                    'x3': float(boxes[i][2][0]),\n                    'y3': float(boxes[i][2][1]),\n                    'x4': float(boxes[i][3][0]),\n                    'y4': float(boxes[i][3][1]),\n                }\n                \n                if angles is not None:\n                    detection['angle'] = float(angles[i])\n                \n                detections.append(detection)\n    \n    return pd.DataFrame(detections)\n\n# Extract detections\ndf_detections = extract_obb_info(test_results)\n\n\nprint(f\"Total detections: {len(df_detections)}\")\nprint(f\"Images with detections: {df_detections['image'].nunique()}\")\nprint(f\"Average confidence: {df_detections['confidence'].mean():.4f}\")\nprint(f\"\\nSample detections:\")\nprint(df_detections.head())\n\n# Save to CSV\ndf_detections.to_csv('/kaggle/working/ship_detections.csv', index=False)\nprint(\"\\n Detections saved to: /kaggle/working/ship_detections.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:05:14.200462Z","iopub.execute_input":"2026-01-28T16:05:14.200789Z","iopub.status.idle":"2026-01-28T16:05:14.284815Z","shell.execute_reply.started":"2026-01-28T16:05:14.200761Z","shell.execute_reply":"2026-01-28T16:05:14.284185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_single_image(image_path, conf_threshold=0.25):\n    \n    # Predict\n    results = model_best.predict(\n        source=image_path,\n        conf=conf_threshold,\n        imgsz=640,\n        save=False\n    )\n    \n    # Visualize\n    result = results[0]\n    img = result.plot()  # Plot predictions on image\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    plt.figure(figsize=(12, 8))\n    plt.imshow(img)\n    plt.title(f'YOLOv26-OBB Prediction (conf > {conf_threshold})')\n    plt.axis('off')\n    plt.show()\n    \n    # Print detection info\n    if result.obb is not None and len(result.obb) > 0:\n        print(f\"\\nDetected {len(result.obb)} ships:\")\n        for i, conf in enumerate(result.obb.conf):\n            print(f\"  Ship {i+1}: confidence = {conf:.3f}\")\n    else:\n        print(\"\\nNo ships detected\")\n    \n    return results\n\n# Test on a random test image\ntest_image = f\"{output_dir}/images/test/{os.listdir(f'{output_dir}/images/test')[0]}\"\nprint(f\"Testing on: {os.path.basename(test_image)}\")\n_ = predict_single_image(test_image, conf_threshold=0.25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:05:19.947145Z","iopub.execute_input":"2026-01-28T16:05:19.947474Z","iopub.status.idle":"2026-01-28T16:05:20.175771Z","shell.execute_reply.started":"2026-01-28T16:05:19.947444Z","shell.execute_reply":"2026-01-28T16:05:20.175012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n Model Performance Analysis \")\n\n# Confidence distribution\nplt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(df_detections['confidence'], bins=30, edgecolor='black', alpha=0.7)\nplt.xlabel('Confidence Score')\nplt.ylabel('Frequency')\nplt.title('Detection Confidence Distribution')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\ndetections_per_image = df_detections.groupby('image').size()\nplt.hist(detections_per_image, bins=20, edgecolor='black', alpha=0.7, color='green')\nplt.xlabel('Number of Ships per Image')\nplt.ylabel('Frequency')\nplt.title('Ships Detected per Image')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Statistics\nprint(f\"\\nConfidence Statistics:\")\nprint(f\"  Mean: {df_detections['confidence'].mean():.4f}\")\nprint(f\"  Median: {df_detections['confidence'].median():.4f}\")\nprint(f\"  Min: {df_detections['confidence'].min():.4f}\")\nprint(f\"  Max: {df_detections['confidence'].max():.4f}\")\n\nprint(f\"\\nDetections per Image:\")\nprint(f\"  Mean: {detections_per_image.mean():.2f}\")\nprint(f\"  Median: {detections_per_image.median():.2f}\")\nprint(f\"  Max: {detections_per_image.max():.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:05:36.432902Z","iopub.execute_input":"2026-01-28T16:05:36.433226Z","iopub.status.idle":"2026-01-28T16:05:36.746716Z","shell.execute_reply.started":"2026-01-28T16:05:36.433199Z","shell.execute_reply":"2026-01-28T16:05:36.746067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n=== Exporting Model ===\")\n\n# Export to ONNX (for deployment)\ntry:\n    onnx_path = model_best.export(format='onnx', imgsz=640)\n    print(f\"âœ“ ONNX model exported to: {onnx_path}\")\nexcept Exception as e:\n    print(f\"ONNX export failed: {e}\")\n\n# Export to TorchScript\ntry:\n    torchscript_path = model_best.export(format='torchscript', imgsz=640)\n    print(f\"âœ“ TorchScript model exported to: {torchscript_path}\")\nexcept Exception as e:\n    print(f\"TorchScript export failed: {e}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ðŸŽ‰ PIPELINE COMPLETE!\")\nprint(\"=\"*60)\nprint(\"\\nNext Steps:\")\nprint(\"1. Analyze training curves in /kaggle/working/runs/ship_yolov26_obb\")\nprint(\"2. Check predictions in /kaggle/working/runs/test_predictions\")\nprint(\"3. Review detections CSV: /kaggle/working/ship_detections.csv\")\nprint(\"4. Integrate with DINOv2 for feature extraction\")\nprint(\"5. Build temporal tracking system\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:05:44.716509Z","iopub.execute_input":"2026-01-28T16:05:44.716831Z"}},"outputs":[],"execution_count":null}]}